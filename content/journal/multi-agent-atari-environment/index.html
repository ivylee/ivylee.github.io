<!DOCTYPE html>
<html data-html-server-rendered="true" lang="en" data-vue-tag="%7B%22lang%22:%7B%22ssr%22:%22en%22%7D%7D">
  <head>
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script
      async
      src="https://www.googletagmanager.com/gtag/js?id=UA-162951235-1"
    ></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() {
        dataLayer.push(arguments);
      }
      gtag("js", new Date());

      gtag("config", "UA-162951235-1");
    </script>

    <title>Multi-agent Atari environment - Ivy Lee</title><meta name="gridsome:hash" content="ffd7038b431585c48d67e3d6878e9f98975f1e8e"><meta data-vue-tag="ssr" charset="utf-8"><meta data-vue-tag="ssr" name="generator" content="Gridsome v0.7.23"><meta data-vue-tag="ssr" data-key="viewport" name="viewport" content="width=device-width, initial-scale=1, viewport-fit=cover"><meta data-vue-tag="ssr" data-key="format-detection" name="format-detection" content="telephone=no"><meta data-vue-tag="ssr" data-key="description" name="description" content="Writings by Ivy Lee, Creative Technologist experienced in Data Science, Machine Learning, Deep Learning"><link data-vue-tag="ssr" rel="icon" href="data:,"><link data-vue-tag="ssr" rel="icon" type="image/png" sizes="16x16" href="/assets/static/favicon.ce0531f.e0811aa2d901a2d517912241f856735c.png"><link data-vue-tag="ssr" rel="icon" type="image/png" sizes="32x32" href="/assets/static/favicon.ac8d93a.e0811aa2d901a2d517912241f856735c.png"><link data-vue-tag="ssr" rel="icon" type="image/png" sizes="96x96" href="/assets/static/favicon.b9532cc.e0811aa2d901a2d517912241f856735c.png"><link data-vue-tag="ssr" rel="apple-touch-icon" type="image/png" sizes="76x76" href="/assets/static/touchicon.f22e9f3.8adfe1442cdabafcdfc4196418110b5a.png"><link data-vue-tag="ssr" rel="apple-touch-icon" type="image/png" sizes="152x152" href="/assets/static/touchicon.62d22cb.8adfe1442cdabafcdfc4196418110b5a.png"><link data-vue-tag="ssr" rel="apple-touch-icon" type="image/png" sizes="120x120" href="/assets/static/touchicon.1539b60.8adfe1442cdabafcdfc4196418110b5a.png"><link data-vue-tag="ssr" rel="apple-touch-icon" type="image/png" sizes="167x167" href="/assets/static/touchicon.dc0cdc5.8adfe1442cdabafcdfc4196418110b5a.png"><link data-vue-tag="ssr" rel="apple-touch-icon" type="image/png" sizes="180x180" href="/assets/static/touchicon.7b22250.8adfe1442cdabafcdfc4196418110b5a.png"><link rel="preload" href="/assets/css/0.styles.2c2daf90.css" as="style"><link rel="preload" href="/assets/js/app.357453b5.js" as="script"><link rel="preload" href="/assets/js/page--src--templates--journal-post-vue.385d9d0f.js" as="script"><link rel="prefetch" href="/assets/js/page--node-modules--gridsome--app--pages--404-vue.286d17b8.js"><link rel="prefetch" href="/assets/js/page--src--pages--index-vue.08f1dacd.js"><link rel="prefetch" href="/assets/js/page--src--pages--journal-vue.0f400f41.js"><link rel="prefetch" href="/assets/js/page--src--templates--project-post-vue.b958d339.js"><link rel="stylesheet" href="/assets/css/0.styles.2c2daf90.css"><noscript data-vue-tag="ssr"><style>.g-image--loading{display:none;}</style></noscript>
  </head>
  <body >
    <div data-server-rendered="true" id="app" class="layout" data-v-354b4fc1><header class="header" data-v-e5cfe63c><div class="container" data-v-e5cfe63c><div class="left" data-v-e5cfe63c><a href="/" class="home-link active" data-v-e5cfe63c><h1 data-v-e5cfe63c>Ivy Lee</h1></a></div><nav class="nav right" data-v-e5cfe63c><a href="/journal" class="nav__link" data-v-e5cfe63c>Posts</a><a href="/resume.pdf" class="nav__link" data-v-e5cfe63c>Resume</a></nav></div></header><div class="journal" data-v-354b4fc1><div class="container journal-container" data-v-354b4fc1><div class="journal-header" data-v-354b4fc1><h1 class="journal-title" data-v-354b4fc1>Multi-agent Atari environment</h1><div class="journal-meta" data-v-354b4fc1><div class="journal-date" data-v-354b4fc1><div data-v-354b4fc1>November 29 2020</div></div></div></div><div class="journal-content" data-v-354b4fc1><h2 id="single-agent-atari-environment"><a href="#single-agent-atari-environment" aria-hidden="true"><span class="icon icon-link"></span></a>Single-agent Atari environment</h2>
<p><a href="https://github.com/openai/gym" target="_blank" rel="nofollow noopener noreferrer">OpenAI Gym</a> provides convenient abstraction of Atari games for training reinforcement learning agents.</p>
<p>An Atari game is represented as a <code>gym.Env</code>. There are two important methods of the <code>gym.Env</code>. The <code>step</code> method takes an action to advance the game into the next state, and returns an observation (typically the image of the console), a reward (the score), a boolean indicating whether the game is over, and an <code>info</code> dictionary with auxiliary information. The <code>reset</code> method resets the Atari game to the original starting state, before any action is taken.</p>
<p>Under the hood, OpenAI Gym depends on <a href="https://github.com/mgbellemare/Arcade-Learning-Environment" target="_blank" rel="nofollow noopener noreferrer">Arcade Learning Environment</a> (ALE), which is a C++ wrapper of <a href="https://stella-emu.github.io/" target="_blank" rel="nofollow noopener noreferrer">Stella emulator</a> and provides a Python interface.</p>
<h2 id="multi-agent-atari-environment"><a href="#multi-agent-atari-environment" aria-hidden="true"><span class="icon icon-link"></span></a>Multi-agent Atari environment</h2>
<p>A multi-agent environment will allow us to study inter-agent dynamics, such as competition and collaboration. How do we go from single-agent Atari environment to multi-agent Atari environment while preserving the <code>gym.Env</code> interface?</p>
<p><a href="https://www.pettingzoo.ml" target="_blank" rel="nofollow noopener noreferrer">PettingZoo</a> has attempted to do just that. The research team behind PettingZoo first modified ALE to enable multi-player support for Atari games, while their <a href="https://github.com/mgbellemare/Arcade-Learning-Environment/pull/350" target="_blank" rel="nofollow noopener noreferrer">pull request</a> is pending to be merged, they published a Python package that can be easily installed <code>pip install multi-agent-ale-py</code>. If you install PettingZoo with Atari - <code>pip install pettingzoo[atari]</code>, Multi-agent ALE is automatically installed as a dependency.</p>
<p>An important design choice PettingZoo has made is to model agents taking action sequentially, as opposed to concurrently (which is native to Atari games). They published the idea in <a href="https://arxiv.org/abs/2009.13051" target="_blank" rel="nofollow noopener noreferrer">Agent Environment Cycle Games</a>. Naturally, someone <a href="https://github.com/PettingZoo-Team/PettingZoo/issues/158" target="_blank" rel="nofollow noopener noreferrer">asked</a> about this, and the PettingZoo team created a wrapper to collect all of the sequentially gathered outputs in one method.</p>
<p>For my work, I don't need AEC games, I actually prefer concurrent actions, and it would be ideal to avoid the intermediate sequential layer. To achieve this, I built a <code>gym.Env</code> like interface with Multi-agent ALE. The <code>step</code> method now takes actions for each player, and returns an observation, rewards for each player and game over booleans for each player. To make it suitable for large scale training with <a href="https://docs.ray.io/en/master/rllib.html" target="_blank" rel="nofollow noopener noreferrer">RLlib</a>, we modify it to extend <code>MultiAgentEnv</code>, create dictionaries for actions and outputs keyed by agent name, and add a boolean to indicate whether the game is over for all of the agents.</p>
<h2 id="final-thoughts"><a href="#final-thoughts" aria-hidden="true"><span class="icon icon-link"></span></a>Final thoughts</h2>
<p>OpenAI Gym created a neat environment interface for training reinforcement learning agents. It is been adopted in PettingZoo and RLlib. Going from single-agent environment to multi-agent environment will enable us to study interesting multi-agent dynamics. RLlib makes large scale training scalable. I'm excited to see more research in the multi-agent RL domain!</p>
</div></div></div><footer class="footer" data-v-4232f071><div class="container" data-v-4232f071><span data-v-4232f071>Copyright Â© 2021</span></div></footer></div> <script>window.__INITIAL_STATE__={"data":{"post":{"title":"Multi-agent Atari environment","date":"November 29 2020","content":"\u003Ch2 id=\"single-agent-atari-environment\"\u003E\u003Ca href=\"#single-agent-atari-environment\" aria-hidden=\"true\"\u003E\u003Cspan class=\"icon icon-link\"\u003E\u003C\u002Fspan\u003E\u003C\u002Fa\u003ESingle-agent Atari environment\u003C\u002Fh2\u003E\n\u003Cp\u003E\u003Ca href=\"https:\u002F\u002Fgithub.com\u002Fopenai\u002Fgym\" target=\"_blank\" rel=\"nofollow noopener noreferrer\"\u003EOpenAI Gym\u003C\u002Fa\u003E provides convenient abstraction of Atari games for training reinforcement learning agents.\u003C\u002Fp\u003E\n\u003Cp\u003EAn Atari game is represented as a \u003Ccode\u003Egym.Env\u003C\u002Fcode\u003E. There are two important methods of the \u003Ccode\u003Egym.Env\u003C\u002Fcode\u003E. The \u003Ccode\u003Estep\u003C\u002Fcode\u003E method takes an action to advance the game into the next state, and returns an observation (typically the image of the console), a reward (the score), a boolean indicating whether the game is over, and an \u003Ccode\u003Einfo\u003C\u002Fcode\u003E dictionary with auxiliary information. The \u003Ccode\u003Ereset\u003C\u002Fcode\u003E method resets the Atari game to the original starting state, before any action is taken.\u003C\u002Fp\u003E\n\u003Cp\u003EUnder the hood, OpenAI Gym depends on \u003Ca href=\"https:\u002F\u002Fgithub.com\u002Fmgbellemare\u002FArcade-Learning-Environment\" target=\"_blank\" rel=\"nofollow noopener noreferrer\"\u003EArcade Learning Environment\u003C\u002Fa\u003E (ALE), which is a C++ wrapper of \u003Ca href=\"https:\u002F\u002Fstella-emu.github.io\u002F\" target=\"_blank\" rel=\"nofollow noopener noreferrer\"\u003EStella emulator\u003C\u002Fa\u003E and provides a Python interface.\u003C\u002Fp\u003E\n\u003Ch2 id=\"multi-agent-atari-environment\"\u003E\u003Ca href=\"#multi-agent-atari-environment\" aria-hidden=\"true\"\u003E\u003Cspan class=\"icon icon-link\"\u003E\u003C\u002Fspan\u003E\u003C\u002Fa\u003EMulti-agent Atari environment\u003C\u002Fh2\u003E\n\u003Cp\u003EA multi-agent environment will allow us to study inter-agent dynamics, such as competition and collaboration. How do we go from single-agent Atari environment to multi-agent Atari environment while preserving the \u003Ccode\u003Egym.Env\u003C\u002Fcode\u003E interface?\u003C\u002Fp\u003E\n\u003Cp\u003E\u003Ca href=\"https:\u002F\u002Fwww.pettingzoo.ml\" target=\"_blank\" rel=\"nofollow noopener noreferrer\"\u003EPettingZoo\u003C\u002Fa\u003E has attempted to do just that. The research team behind PettingZoo first modified ALE to enable multi-player support for Atari games, while their \u003Ca href=\"https:\u002F\u002Fgithub.com\u002Fmgbellemare\u002FArcade-Learning-Environment\u002Fpull\u002F350\" target=\"_blank\" rel=\"nofollow noopener noreferrer\"\u003Epull request\u003C\u002Fa\u003E is pending to be merged, they published a Python package that can be easily installed \u003Ccode\u003Epip install multi-agent-ale-py\u003C\u002Fcode\u003E. If you install PettingZoo with Atari - \u003Ccode\u003Epip install pettingzoo[atari]\u003C\u002Fcode\u003E, Multi-agent ALE is automatically installed as a dependency.\u003C\u002Fp\u003E\n\u003Cp\u003EAn important design choice PettingZoo has made is to model agents taking action sequentially, as opposed to concurrently (which is native to Atari games). They published the idea in \u003Ca href=\"https:\u002F\u002Farxiv.org\u002Fabs\u002F2009.13051\" target=\"_blank\" rel=\"nofollow noopener noreferrer\"\u003EAgent Environment Cycle Games\u003C\u002Fa\u003E. Naturally, someone \u003Ca href=\"https:\u002F\u002Fgithub.com\u002FPettingZoo-Team\u002FPettingZoo\u002Fissues\u002F158\" target=\"_blank\" rel=\"nofollow noopener noreferrer\"\u003Easked\u003C\u002Fa\u003E about this, and the PettingZoo team created a wrapper to collect all of the sequentially gathered outputs in one method.\u003C\u002Fp\u003E\n\u003Cp\u003EFor my work, I don't need AEC games, I actually prefer concurrent actions, and it would be ideal to avoid the intermediate sequential layer. To achieve this, I built a \u003Ccode\u003Egym.Env\u003C\u002Fcode\u003E like interface with Multi-agent ALE. The \u003Ccode\u003Estep\u003C\u002Fcode\u003E method now takes actions for each player, and returns an observation, rewards for each player and game over booleans for each player. To make it suitable for large scale training with \u003Ca href=\"https:\u002F\u002Fdocs.ray.io\u002Fen\u002Fmaster\u002Frllib.html\" target=\"_blank\" rel=\"nofollow noopener noreferrer\"\u003ERLlib\u003C\u002Fa\u003E, we modify it to extend \u003Ccode\u003EMultiAgentEnv\u003C\u002Fcode\u003E, create dictionaries for actions and outputs keyed by agent name, and add a boolean to indicate whether the game is over for all of the agents.\u003C\u002Fp\u003E\n\u003Ch2 id=\"final-thoughts\"\u003E\u003Ca href=\"#final-thoughts\" aria-hidden=\"true\"\u003E\u003Cspan class=\"icon icon-link\"\u003E\u003C\u002Fspan\u003E\u003C\u002Fa\u003EFinal thoughts\u003C\u002Fh2\u003E\n\u003Cp\u003EOpenAI Gym created a neat environment interface for training reinforcement learning agents. It is been adopted in PettingZoo and RLlib. Going from single-agent environment to multi-agent environment will enable us to study interesting multi-agent dynamics. RLlib makes large scale training scalable. I'm excited to see more research in the multi-agent RL domain!\u003C\u002Fp\u003E\n"}},"context":{}};(function(){var s;(s=document.currentScript||document.scripts[document.scripts.length-1]).parentNode.removeChild(s);}());</script><script src="/assets/js/app.357453b5.js" defer></script><script src="/assets/js/page--src--templates--journal-post-vue.385d9d0f.js" defer></script>
  </body>
</html>
